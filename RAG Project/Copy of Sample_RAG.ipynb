{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"d45nLMGS5Q4M","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730968389732,"user_tz":-330,"elapsed":22793,"user":{"displayName":"Siddharth Deep Learning","userId":"08603031256831117826"}},"outputId":"2d5623c3-00ad-4add-fd45-c2ea9adb9a85"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting streamlit\n","  Downloading streamlit-1.40.0-py2.py3-none-any.whl.metadata (8.5 kB)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n","Requirement already satisfied: langchain in /usr/local/lib/python3.10/dist-packages (0.3.4)\n","Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.10/dist-packages (3.2.1)\n","Collecting faiss-gpu\n","  Downloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.4 kB)\n","Collecting langchain-community\n","  Downloading langchain_community-0.3.5-py3-none-any.whl.metadata (2.9 kB)\n","Collecting pypdf\n","  Downloading pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n","Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n","Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.5.0)\n","Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n","Requirement already satisfied: numpy<3,>=1.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.26.4)\n","Requirement already satisfied: packaging<25,>=20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.1)\n","Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.2.2)\n","Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (10.4.0)\n","Requirement already satisfied: protobuf<6,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n","Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (17.0.0)\n","Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.32.3)\n","Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.9.3)\n","Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.0.0)\n","Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n","Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.2)\n","Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.1.43)\n","Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n","  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n","Requirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n","Collecting watchdog<6,>=2.1.5 (from streamlit)\n","  Downloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n","Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.6)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.36)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.10)\n","Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n","Requirement already satisfied: langchain-core<0.4.0,>=0.3.12 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.13)\n","Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.3.0)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /usr/local/lib/python3.10/dist-packages (from langchain) (0.1.137)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n","Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (2.5.0+cu121)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.5.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence_transformers) (1.13.1)\n","Collecting SQLAlchemy<3,>=1.4 (from langchain)\n","  Downloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.6 kB)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting httpx-sse<0.5.0,>=0.4.0 (from langchain-community)\n","  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n","Collecting langchain\n","  Downloading langchain-0.3.7-py3-none-any.whl.metadata (7.1 kB)\n","Collecting langchain-core<0.4.0,>=0.3.12 (from langchain)\n","  Downloading langchain_core-0.3.15-py3-none-any.whl.metadata (6.3 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n","  Downloading pydantic_settings-2.6.1-py3-none-any.whl.metadata (3.5 kB)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n","Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.17.0)\n","Requirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n","Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.23.0)\n","Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading marshmallow-3.23.1-py3-none-any.whl.metadata (7.5 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.11)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.10.0)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.12->langchain) (1.33)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (0.27.2)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (3.10.10)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.4.0->streamlit) (2024.2)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.4.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.2.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.8.30)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.18.0)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (3.4.2)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (1.4.2)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence_transformers) (3.5.0)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.1)\n","Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (3.7.1)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.0.6)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.3.1)\n","Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (0.14.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (3.0.2)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.12->langchain) (3.0.0)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2024.10.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.20.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.16.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading mypy_extensions-1.0.0-py3-none-any.whl.metadata (1.1 kB)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain) (1.2.2)\n","Downloading streamlit-1.40.0-py2.py3-none-any.whl (8.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m92.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (85.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m27.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_community-0.3.5-py3-none-any.whl (2.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain-0.3.7-py3-none-any.whl (1.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m62.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.0/298.0 kB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n","Downloading langchain_core-0.3.15-py3-none-any.whl (408 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.7/408.7 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pydantic_settings-2.6.1-py3-none-any.whl (28 kB)\n","Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m117.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading SQLAlchemy-2.0.35-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m101.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading watchdog-5.0.3-py3-none-manylinux2014_x86_64.whl (79 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.3/79.3 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.23.1-py3-none-any.whl (49 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.5/49.5 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n","Installing collected packages: faiss-gpu, watchdog, SQLAlchemy, python-dotenv, pypdf, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydeck, pydantic-settings, dataclasses-json, langchain-core, streamlit, langchain, langchain-community\n","  Attempting uninstall: SQLAlchemy\n","    Found existing installation: SQLAlchemy 2.0.36\n","    Uninstalling SQLAlchemy-2.0.36:\n","      Successfully uninstalled SQLAlchemy-2.0.36\n","  Attempting uninstall: langchain-core\n","    Found existing installation: langchain-core 0.3.13\n","    Uninstalling langchain-core-0.3.13:\n","      Successfully uninstalled langchain-core-0.3.13\n","  Attempting uninstall: langchain\n","    Found existing installation: langchain 0.3.4\n","    Uninstalling langchain-0.3.4:\n","      Successfully uninstalled langchain-0.3.4\n","Successfully installed SQLAlchemy-2.0.35 dataclasses-json-0.6.7 faiss-gpu-1.7.2 httpx-sse-0.4.0 langchain-0.3.7 langchain-community-0.3.5 langchain-core-0.3.15 marshmallow-3.23.1 mypy-extensions-1.0.0 pydantic-settings-2.6.1 pydeck-0.9.1 pypdf-5.1.0 python-dotenv-1.0.1 streamlit-1.40.0 typing-inspect-0.9.0 watchdog-5.0.3\n"]}],"source":["!pip install streamlit transformers langchain sentence_transformers faiss-gpu langchain-community pypdf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yEifXyaQ9Fxc","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730968404022,"user_tz":-330,"elapsed":2019,"user":{"displayName":"Siddharth Deep Learning","userId":"08603031256831117826"}},"outputId":"0a55b4ee-c784-45a7-fe6e-9f07d3a31914"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pyngrok\n","  Downloading pyngrok-7.2.1-py3-none-any.whl.metadata (8.3 kB)\n","Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.10/dist-packages (from pyngrok) (6.0.2)\n","Downloading pyngrok-7.2.1-py3-none-any.whl (22 kB)\n","Installing collected packages: pyngrok\n","Successfully installed pyngrok-7.2.1\n"]}],"source":["!pip install --upgrade pyngrok"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U9auok60C3BF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730968414257,"user_tz":-330,"elapsed":10239,"user":{"displayName":"Siddharth Deep Learning","userId":"08603031256831117826"}},"outputId":"d8c672e4-2d29-476a-cb37-fc7b5091dad7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting nbimporter\n","  Downloading nbimporter-0.3.4-py3-none-any.whl.metadata (252 bytes)\n","Downloading nbimporter-0.3.4-py3-none-any.whl (4.9 kB)\n","Installing collected packages: nbimporter\n","Successfully installed nbimporter-0.3.4\n","Collecting bitsandbytes\n","  Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl.metadata (3.5 kB)\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (2.5.0+cu121)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.26.4)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.16.1)\n","Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (3.1.4)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (2024.10.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->bitsandbytes) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->bitsandbytes) (1.3.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->bitsandbytes) (3.0.2)\n","Downloading bitsandbytes-0.44.1-py3-none-manylinux_2_24_x86_64.whl (122.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.4/122.4 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: bitsandbytes\n","Successfully installed bitsandbytes-0.44.1\n"]}],"source":["!pip install nbimporter\n","!pip install -U bitsandbytes"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h3lXhhSb5KwP"},"outputs":[],"source":["import os\n","import streamlit as st\n","import requests\n","from typing import List\n","from bs4 import BeautifulSoup\n","from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline, BitsAndBytesConfig, GenerationConfig\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import FAISS\n","from langchain.document_loaders import PyPDFLoader\n","from langchain.chains import RetrievalQA\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain import HuggingFacePipeline\n","import torch\n","import re\n","from typing import List, Dict, Set, Optional, Tuple"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IaLtRSUs5oOT"},"outputs":[],"source":["def generate_citation(metadata, format=\"APA\"):\n","    \"\"\"\n","    Generate citations in multiple formats based on provided metadata.\n","\n","    Args:\n","    - metadata (dict): Document metadata containing authors, title, etc.\n","    - format (str): Citation format (APA, MLA, Chicago, BibTeX, or RIS)\n","\n","    Returns:\n","    - str: Formatted citation string\n","    \"\"\"\n","    # Extract and clean metadata\n","    authors = metadata.get('authors', ['Unknown Author'])\n","    title = metadata.get('title', 'Untitled').strip()\n","    year = str(metadata.get('publication_date', {}).get('year', 'n.d.'))\n","    journal = metadata.get('journal', 'Unknown Journal').strip()\n","    doi = metadata.get('doi', '')\n","\n","    # Format authors for different citation styles\n","    def format_authors_apa():\n","        if len(authors) == 1:\n","            return authors[0]\n","        elif len(authors) == 2:\n","            return f\"{authors[0]} & {authors[1]}\"\n","        elif len(authors) > 2:\n","            return f\"{authors[0]} et al.\"\n","\n","    def format_authors_mla():\n","        if len(authors) == 1:\n","            return authors[0]\n","        elif len(authors) == 2:\n","            return f\"{authors[0]} and {authors[1]}\"\n","        elif len(authors) > 2:\n","            return f\"{authors[0]} et al.\"\n","\n","    def format_authors_chicago():\n","        if len(authors) == 1:\n","            return authors[0]\n","        elif len(authors) == 2:\n","            return f\"{authors[0]} and {authors[1]}\"\n","        elif len(authors) > 2:\n","            return f\"{authors[0]} et al.\"\n","\n","    def format_authors_bibtex():\n","        return \" and \".join(authors)\n","\n","    # Generate citations based on format\n","    if format.upper() == \"APA\":\n","        authors_formatted = format_authors_apa()\n","        return f\"{authors_formatted}. ({year}). {title}. {journal}. {doi}\"\n","\n","    elif format.upper() == \"MLA\":\n","        authors_formatted = format_authors_mla()\n","        return f\"{authors_formatted}. \\\"{title}.\\\" {journal}, {year}, {doi}.\"\n","\n","    elif format.upper() == \"CHICAGO\":\n","        authors_formatted = format_authors_chicago()\n","        return f\"{authors_formatted}. {year}. \\\"{title}.\\\" {journal}. {doi}.\"\n","\n","    elif format.upper() == \"BIBTEX\":\n","        authors_formatted = format_authors_bibtex()\n","        # Create a citation key using first author's lastname and year\n","        first_author_lastname = authors[0].split()[-1].lower()\n","        citation_key = f\"{first_author_lastname}{year}\"\n","\n","        bibtex_template = \"@article{\" + citation_key + \",\\n\"\n","        bibtex_template += \"    author = {\" + authors_formatted + \"},\\n\"\n","        bibtex_template += \"    title = {\" + title + \"},\\n\"\n","        bibtex_template += \"    journal = {\" + journal + \"},\\n\"\n","        bibtex_template += \"    year = {\" + year + \"},\\n\"\n","        bibtex_template += \"    doi = {\" + doi + \"}\\n\"\n","        bibtex_template += \"}\"\n","        return bibtex_template\n","\n","    elif format.upper() == \"RIS\":\n","        ris_template = \"TY  - JOUR\\n\"\n","        for author in authors:\n","            ris_template += f\"AU  - {author}\\n\"\n","        ris_template += f\"TI  - {title}\\n\"\n","        ris_template += f\"JO  - {journal}\\n\"\n","        ris_template += f\"PY  - {year}\\n\"\n","        ris_template += f\"DO  - {doi}\\n\"\n","        ris_template += \"ER  -\"\n","        return ris_template\n","\n","    else:\n","        return \"Unsupported citation format\""]},{"cell_type":"code","source":["import urllib.parse\n","import requests\n","import xml.etree.ElementTree as ET\n","from datetime import datetime\n","from typing import Dict, Any\n","\n","class PaperMetadata:\n","    def __init__(self):\n","        self.arxiv_base_url = \"http://export.arxiv.org/api/query\"\n","        self.semantic_scholar_base_url = \"https://api.semanticscholar.org/v1/paper/arXiv:\"\n","        # New: Adding S2 API v2 endpoint which sometimes has more complete metadata\n","        self.s2_api_v2_base = \"https://api.semanticscholar.org/graph/v1/paper/arXiv:\"\n","\n","    def get_paper_metadata(self, pdf_url: str) -> Dict[str, Any]:\n","        \"\"\"Extract metadata from arXiv and both versions of Semantic Scholar API\"\"\"\n","        try:\n","            # Extract arxiv ID\n","            if 'arxiv.org/pdf/' in pdf_url:\n","                arxiv_id = pdf_url.split('arxiv.org/pdf/')[1].replace('.pdf', '')\n","            elif 'arxiv.org/abs/' in pdf_url:\n","                arxiv_id = pdf_url.split('arxiv.org/abs/')[1]\n","            else:\n","                return {\"error\": \"Invalid arXiv URL\"}\n","\n","            # Remove version number for Semantic Scholar\n","            base_arxiv_id = arxiv_id.split('v')[0]\n","\n","            # Get metadata from all sources\n","            arxiv_data = self._fetch_arxiv_metadata(arxiv_id)\n","            semantic_data = self._fetch_semantic_scholar_metadata(base_arxiv_id)\n","            s2_v2_data = self._fetch_s2_v2_metadata(base_arxiv_id)\n","\n","            # Combine metadata with priority\n","            return self._combine_metadata(arxiv_data, semantic_data, s2_v2_data)\n","\n","        except Exception as e:\n","            return {\"error\": f\"An error occurred: {str(e)}\"}\n","\n","    def _fetch_arxiv_metadata(self, arxiv_id: str) -> Dict[str, Any]:\n","        \"\"\"Fetch metadata from arXiv API\"\"\"\n","        api_url = f\"{self.arxiv_base_url}?id_list={arxiv_id}\"\n","        response = requests.get(api_url)\n","        response.raise_for_status()\n","\n","        root = ET.fromstring(response.content)\n","        namespace = {\n","            'atom': 'http://www.w3.org/2005/Atom',\n","            'arxiv': 'http://arxiv.org/schemas/atom'\n","        }\n","\n","        entry = root.find('atom:entry', namespace)\n","        if entry is None:\n","            return {}\n","\n","        published = entry.find('atom:published', namespace).text\n","        pub_date = datetime.strptime(published, '%Y-%m-%dT%H:%M:%SZ')\n","\n","        journal_ref = entry.find('arxiv:journal_ref', namespace)\n","        doi = entry.find('arxiv:doi', namespace)\n","\n","        return {\n","            \"arxiv_id\": arxiv_id,\n","            \"title\": entry.find('atom:title', namespace).text.strip(),\n","            \"authors\": [author.find('atom:name', namespace).text\n","                       for author in entry.findall('atom:author', namespace)],\n","            \"abstract\": entry.find('atom:summary', namespace).text.strip(),\n","            \"publication_date\": {\n","                \"full\": pub_date.isoformat(),\n","                \"year\": pub_date.year,\n","                \"month\": pub_date.month,\n","                \"formatted\": pub_date.strftime('%B %Y')\n","            },\n","            \"categories\": [category.attrib['term']\n","                          for category in entry.findall('atom:category', namespace)],\n","            \"primary_category\": entry.find('arxiv:primary_category', namespace).attrib['term'],\n","            \"journal_ref\": journal_ref.text if journal_ref is not None else None,\n","            \"doi\": doi.text if doi is not None else None\n","        }\n","\n","    def _fetch_semantic_scholar_metadata(self, arxiv_id: str) -> Dict[str, Any]:\n","        \"\"\"Fetch metadata from Semantic Scholar API v1\"\"\"\n","        try:\n","            response = requests.get(f\"{self.semantic_scholar_base_url}{arxiv_id}\")\n","            if response.status_code == 200:\n","                data = response.json()\n","                return {\n","                    \"doi\": data.get(\"doi\"),\n","                    \"journal\": data.get(\"venue\"),\n","                    \"volume\": data.get(\"volume\"),\n","                    \"year\": data.get(\"year\")\n","                }\n","            return {}\n","        except:\n","            return {}\n","\n","    def _fetch_s2_v2_metadata(self, arxiv_id: str) -> Dict[str, Any]:\n","        \"\"\"Fetch metadata from Semantic Scholar API v2\"\"\"\n","        try:\n","            fields = \"venue,year,volume,publicationVenue,publicationTypes,doi\"\n","            response = requests.get(\n","                f\"{self.s2_api_v2_base}{arxiv_id}?fields={fields}\"\n","            )\n","            if response.status_code == 200:\n","                data = response.json()\n","                venue_info = data.get(\"publicationVenue\", {})\n","                return {\n","                    \"doi\": data.get(\"doi\"),\n","                    \"journal\": venue_info.get(\"name\") or data.get(\"venue\"),\n","                    \"volume\": venue_info.get(\"volume\") or data.get(\"volume\"),\n","                    \"year\": data.get(\"year\"),\n","                    \"publication_types\": data.get(\"publicationTypes\", [])\n","                }\n","            return {}\n","        except:\n","            return {}\n","\n","    def _combine_metadata(self, arxiv: Dict, semantic: Dict, s2_v2: Dict) -> Dict:\n","        \"\"\"Combine metadata from all sources\"\"\"\n","        combined = arxiv.copy()\n","\n","        # Update with Semantic Scholar v1 data\n","        if semantic:\n","            if semantic.get(\"doi\"):\n","                combined[\"doi\"] = semantic[\"doi\"]\n","            if semantic.get(\"journal\"):\n","                combined[\"journal\"] = semantic[\"journal\"]\n","            if semantic.get(\"volume\"):\n","                combined[\"volume\"] = semantic[\"volume\"]\n","\n","        # Update with Semantic Scholar v2 data (highest priority)\n","        if s2_v2:\n","            if s2_v2.get(\"doi\"):\n","                combined[\"doi\"] = s2_v2[\"doi\"]\n","            if s2_v2.get(\"journal\"):\n","                combined[\"journal\"] = s2_v2[\"journal\"]\n","            if s2_v2.get(\"volume\"):\n","                combined[\"volume\"] = s2_v2[\"volume\"]\n","            if s2_v2.get(\"publication_types\"):\n","                combined[\"publication_types\"] = s2_v2[\"publication_types\"]\n","\n","        # If we still don't have a journal name but have journal_ref, try to parse it\n","        if not combined.get(\"journal\") and combined.get(\"journal_ref\"):\n","            combined[\"journal\"] = combined[\"journal_ref\"].split(\",\")[0]\n","\n","        # Special handling for known conferences/journals\n","        if combined.get(\"journal\") == \"Neural Information Processing Systems\":\n","            combined[\"journal\"] = \"NeurIPS\"  # More commonly used name\n","            if not combined.get(\"doi\") and combined.get(\"publication_date\"):\n","                # Try to construct DOI for NeurIPS papers\n","                year = combined[\"publication_date\"][\"year\"]\n","                combined[\"doi\"] = f\"10.5555/nips.{year}\"  # Note: This is a placeholder DOI pattern\n","\n","        return combined"],"metadata":{"id":"-hf48bVQ4SIe"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1v_HnTR3Fmty"},"outputs":[],"source":["def extract_references(text: str) -> List[str]:\n","    \"\"\"\n","    Extract references from text with improved pattern matching.\n","    Returns list of individual references.\n","    \"\"\"\n","    # Common variations of reference section headers\n","    reference_headers = [\n","        \"References\", \"REFERENCES\",\n","        \"Bibliography\", \"BIBLIOGRAPHY\",\n","        \"Works Cited\", \"WORKS CITED\"\n","    ]\n","\n","    # Find where references section starts\n","    start_idx = -1\n","    for header in reference_headers:\n","        if header in text:\n","            start_idx = text.find(header)\n","            break\n","\n","    if start_idx == -1:\n","        return []\n","\n","    references_text = text[start_idx:].strip()\n","\n","    # Enhanced pattern to match different reference formats\n","    reference_patterns = [\n","        # [1] Author et al. Title. Journal, Year.\n","        r'\\[\\d+\\][^[]+?(?=\\[\\d+\\]|$)',\n","\n","        # 1. Author et al. Title. Journal, Year.\n","        r'\\d+\\.\\s+[^.]+?(?=\\d+\\.\\s+|$)',\n","\n","        # [Author1, Author2] Title. Journal, Year.\n","        r'\\[[^\\]]+\\]\\s+[^[]+?(?=\\[|$)',\n","\n","        # Author et al. (Year) Title. Journal.\n","        r'[A-Z][^.]+?\\(\\d{4}\\)[^.]+?(?=\\s+[A-Z]|$)'\n","    ]\n","\n","    references = []\n","    for pattern in reference_patterns:\n","        matches = re.findall(pattern, references_text)\n","        if matches:\n","            references.extend(matches)\n","            break  # Use the first successful pattern\n","\n","    # Clean up references\n","    cleaned_refs = []\n","    for ref in references:\n","        # Remove extra whitespace and line breaks\n","        cleaned = ' '.join(ref.split())\n","        # Remove standalone numbers that might appear at the start of lines\n","        cleaned = re.sub(r'^\\d+\\s+', '', cleaned)\n","        if cleaned:\n","            cleaned_refs.append(cleaned)\n","\n","    return cleaned_refs\n","\n","def extract_citations(text: str) -> List[int]:\n","    \"\"\"\n","    Extract citation numbers from text.\n","    Returns a list of integers representing citation numbers, including duplicates.\n","    \"\"\"\n","    citations = []\n","\n","    # Pattern for different citation formats\n","    patterns = [\n","        r'\\[\\s*(\\d+)\\s*\\]',                    # [1]\n","        r'\\[\\s*(\\d+(?:\\s*,\\s*\\d+)*)\\s*\\]',     # [1,2,3]\n","        r'\\[\\s*(\\d+)\\s*-\\s*(\\d+)\\s*\\]'         # [1-3]\n","    ]\n","\n","    for pattern in patterns:\n","        matches = re.finditer(pattern, text)\n","        for match in matches:\n","            if '-' in match.group(0):  # Handle range format\n","                start, end = map(int, re.findall(r'\\d+', match.group(0)))\n","                citations.extend(range(start, end + 1))\n","            else:  # Handle single number or comma-separated numbers\n","                numbers = re.findall(r'\\d+', match.group(0))\n","                citations.extend(map(int, numbers))\n","\n","    return list(dict.fromkeys(citations))\n","\n","\n","def is_reference_continuation(text: str, prev_refs: List[str]) -> bool:\n","    \"\"\"\n","    Enhanced check if the current page is a continuation of references.\n","    \"\"\"\n","    # Remove common header/footer text that might appear\n","    text = re.sub(r'^.*?(?:page|p\\.)\\s+\\d+.*$', '', text, flags=re.MULTILINE | re.IGNORECASE)\n","\n","    # Check if page starts with a reference-like pattern\n","    ref_start_patterns = [\n","        r'^\\s*\\[\\d+\\]',  # [1] format\n","        r'^\\s*\\d+\\.',    # 1. format\n","        r'^\\s*[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s*(?:et\\s+al\\.)?,',  # Author name pattern\n","        r'^\\s*[A-Z][a-z]+(?:\\s+[A-Z][a-z]+)*\\s*\\(',  # Author name with year\n","        r'^\\s*\\[',       # [Author] format\n","        r'.*?(?:19|20)\\d{2}.*?(?:journal|conference|proceedings|arxiv)',  # Contains year and venue\n","    ]\n","\n","    # If we have previous references, check if the last one was incomplete\n","    if prev_refs:\n","        last_ref = prev_refs[-1].strip()\n","        if not last_ref.endswith('.'):\n","            return True\n","        if re.search(r'\\([12]\\d{3}[a-z]?\\)$', last_ref):  # Ends with year\n","            return True\n","\n","    # Check if page starts with a reference pattern\n","    text_start = text.strip()[:200]  # Check first 200 chars\n","    for pattern in ref_start_patterns:\n","        if re.search(pattern, text_start, re.MULTILINE):\n","            return True\n","\n","    # Check for continuation indicators\n","    continuation_indicators = [\n","        r'^\\s*(?:and|&)',  # Line starts with connecting words\n","        r'^\\s*[a-z]',      # Continues with lowercase (middle of sentence)\n","        r'^\\s*(?:pp\\.|vol\\.|pages|chapter)',  # Publication details\n","    ]\n","\n","    for pattern in continuation_indicators:\n","        if re.search(pattern, text_start, re.MULTILINE):\n","            return True\n","\n","    return False\n","\n","def process_references_in_content(documents):\n","    \"\"\"\n","    Process references and citations across all documents with improved continuation handling.\n","    Once references start on a page, all subsequent pages are treated as potential reference pages.\n","    \"\"\"\n","    references_found = False\n","    accumulated_references = []\n","    reference_pages = set()\n","    reference_start_page = -1\n","\n","    # First pass: identify where references section starts\n","    for i, doc in enumerate(documents):\n","        page_content = doc.page_content\n","\n","        # Extract citations for all pages\n","        citations = extract_citations(page_content)\n","        if citations:\n","            doc.metadata[\"citations\"] = citations\n","\n","        # Check for reference section start\n","        if not references_found:\n","            refs = extract_references(page_content)\n","            if refs:\n","                references_found = True\n","                reference_start_page = i\n","                accumulated_references.extend(refs)\n","                reference_pages.add(i)\n","                doc.metadata[\"references\"] = refs\n","                doc.metadata[\"is_reference_page\"] = True\n","\n","    # If we found references, process all subsequent pages\n","    if reference_start_page != -1:\n","        for i in range(reference_start_page + 1, len(documents)):\n","            page_content = documents[i].page_content\n","\n","            # Try to extract references from the page content\n","            new_refs = []\n","\n","            # First try with standard reference extraction\n","            refs = extract_references(page_content)\n","            if refs:\n","                new_refs.extend(refs)\n","\n","            # If no references found with standard extraction, try parsing the raw content\n","            if not new_refs:\n","                # Split content by possible reference delimiters\n","                lines = re.split(r'(?:\\[\\d+\\]|\\b\\d+\\.\\s+)', page_content)\n","                lines = [line.strip() for line in lines if line.strip()]\n","\n","                # Process each line as a potential reference\n","                for line in lines:\n","                    # Basic validation: check if line looks like a reference\n","                    if (re.search(r'\\(\\d{4}\\)', line) or  # Has a year in parentheses\n","                        re.search(r'\\b(?:19|20)\\d{2}\\b', line) or  # Has a year\n","                        re.search(r'(?:journal|proceedings|conference|arxiv)', line.lower())):  # Has publication venue\n","                        new_refs.append(line)\n","\n","            if new_refs:\n","                accumulated_references.extend(new_refs)\n","                reference_pages.add(i)\n","                documents[i].metadata[\"references\"] = new_refs\n","                documents[i].metadata[\"is_reference_page\"] = True\n","            else:\n","                # Even if no new references found, mark as reference page if it contains\n","                # text that looks like a continuation\n","                if is_reference_continuation(page_content, accumulated_references):\n","                    reference_pages.add(i)\n","                    documents[i].metadata[\"is_reference_page\"] = True\n","                    documents[i].metadata[\"references\"] = []\n","\n","    # Final pass: update all reference pages with complete reference list\n","    complete_references = list(dict.fromkeys(accumulated_references))  # Remove duplicates\n","    for i in reference_pages:\n","        documents[i].metadata[\"all_references\"] = complete_references\n","        documents[i].metadata[\"total_references\"] = len(complete_references)\n","\n","    return documents\n","\n","def load_pdfs(pdf_path: str):\n","    \"\"\"\n","    Load and process a single PDF file with enhanced reference and citation extraction.\n","    \"\"\"\n","    try:\n","        loader = PyPDFLoader(pdf_path)\n","        documents = loader.load()\n","\n","        # Process references and citations\n","        documents = process_references_in_content(documents)\n","\n","        return documents\n","\n","    except Exception as e:\n","        print(f\"Error processing PDF {pdf_path}: {str(e)}\")\n","        return []"]},{"cell_type":"code","source":["data_dir = \"/content/drive/MyDrive/RAG Project/Data/sample_paper.pdf\"  # Update this path to your actual data directory\n","documents = load_pdfs(data_dir)"],"metadata":{"id":"iwuWIgu0AuHv"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(documents[0])"],"metadata":{"id":"OoO9UI07A5LP"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xT1qz96p5yBF","colab":{"base_uri":"https://localhost:8080/","height":121,"referenced_widgets":["647a0920ab9344c49bdba3a1a1c4ecde","4857b7e81d3b440b800b56f8ba0fc10d","e77d334e3052478ba1f7cd5ff6fa97b5","4bab21799b05459c89813656af8c9518","cb5c5c02c0764dbcbb809b0e8e4d8448","176da66791014f8cb214145f7fb6d087","444a50209cc041f8a52e8a10bacf8412","c36b58d75951460894f0997f76b7a683","7deda925f3e44c139433cbc7acbc43d2","356b86620b2848748bcccc9fad9887bb","4a7bba48d71141c2a5065d82037c53cb"]},"executionInfo":{"status":"ok","timestamp":1729931617346,"user_tz":-330,"elapsed":9437,"user":{"displayName":"Siddharth Deep Learning","userId":"08603031256831117826"}},"outputId":"99d7055f-f682-45aa-ff50-a28c7d6e0b53"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","Unused kwargs: ['bnb_8bit_compute_dtype', 'bnb_8bit_quant_type', 'bnb_8bit_use_double_quant']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"]},{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"647a0920ab9344c49bdba3a1a1c4ecde"}},"metadata":{}}],"source":["# -----------------------------------------------------------------------------\n","# RAG System Setup: Embedding Models, Document Loader, Text Splitter, QA System\n","# -----------------------------------------------------------------------------\n","\n","# Initialize Hugging Face API for embeddings and LLMs\n","os.environ[\"HUGGINGFACE_API_TOKEN\"] = \"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\"\n","\n","# Load the embedding model\n","embed_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n","\n","# Function to load PDFs\n","def load_pdfs(pdf_path: str):\n","    \"\"\"\n","    Load and process a single PDF file with enhanced reference and citation extraction.\n","    \"\"\"\n","    try:\n","        loader = PyPDFLoader(pdf_path)\n","        documents = loader.load()\n","\n","        # Process references and citations\n","        documents = process_references_in_content(documents)\n","\n","        return documents\n","\n","    except Exception as e:\n","        print(f\"Error processing PDF {pdf_path}: {str(e)}\")\n","        return []\n","\n","# Define text splitter for chunking documents with section-based heuristics\n","class SectionBasedTextSplitter(RecursiveCharacterTextSplitter):\n","    def __init__(self, chunk_size=1000, chunk_overlap=200, **kwargs):\n","        # Call the parent class without the 'separator' argument\n","        super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap, **kwargs)\n","\n","    def split_text(self, text: str) -> List[str]:\n","        # Split text based on section headings to keep related content together\n","        sections = re.split(r'\\n\\s*(?:Introduction|Methods|Methodology|Results|Discussion|Conclusion|References)\\s*\\n', text, flags=re.IGNORECASE)\n","        chunks = []\n","        for section in sections:\n","            # Further split the section into chunks using the base class logic\n","            section_chunks = super().split_text(section)\n","            chunks.extend(section_chunks)\n","        return chunks\n","\n","# Initialize the custom text splitter without 'separator' argument\n","text_splitter = SectionBasedTextSplitter(\n","    chunk_size=1500,  # Increased chunk size\n","    chunk_overlap=300  # Increased overlap\n",")\n","\n","quantization_config_4bit = BitsAndBytesConfig(\n","    load_in_4bit=True,\n","    bnb_4bit_compute_dtype=torch.float16,\n","    bnb_4bit_quant_type=\"nf4\",\n","    bnb_4bit_use_double_quant=True,\n",")\n","\n","quantization_config_8bit = BitsAndBytesConfig(\n","    load_in_8bit=True,\n","    bnb_8bit_compute_dtype=torch.float16,\n","    bnb_8bit_quant_type=\"nf4\",\n","    bnb_8bit_use_double_quant=True,\n",")\n","\n","# Initialize the Llama model from Hugging Face\n","model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n","tokenizer = AutoTokenizer.from_pretrained(model_id, token=os.environ[\"HUGGINGFACE_API_TOKEN\"])\n","model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config_4bit, torch_dtype=torch.float16, device_map=\"auto\", token=os.environ[\"HUGGINGFACE_API_TOKEN\"])\n","pipe = pipeline(\n","    \"text-generation\",\n","    model=model,\n","    tokenizer=tokenizer,\n","    max_length=4096,\n","    temperature=0.2,\n","    top_p=0.95,\n","    repetition_penalty=1.15\n",")\n","\n","# Create a LangChain wrapper for the pipeline\n","llm = HuggingFacePipeline(pipeline=pipe)\n","\n","# Create FAISS vector store and Retrieval-based QA system\n","def setup_qa_chain(documents):\n","    splits = text_splitter.split_documents(documents)\n","    vectorstore = FAISS.from_documents(splits, embed_model)\n","    qa_chain = RetrievalQA.from_chain_type(\n","        llm=llm,\n","        chain_type=\"stuff\",\n","        retriever=vectorstore.as_retriever(search_kwargs={\"k\": 5}),\n","        return_source_documents=True,\n","    )\n","    return qa_chain"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vv5o2tIyI9SG"},"outputs":[],"source":["data_dir = \"/content/drive/MyDrive/RAG Project/Data/sample_paper.pdf\"  # Update this path to your actual data directory\n","documents = load_pdfs(data_dir)\n","qa_chain = setup_qa_chain(documents)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OTM1STg39xAb"},"outputs":[],"source":["import re\n","from typing import List\n","from langchain.schema import Document\n","\n","def summarize_text(text: str, llm_model) -> str:\n","    \"\"\"\n","    Summarize the given text using the provided LLM model.\n","    Returns a summary of approximately 100 words.\n","    \"\"\"\n","    prompt = f\"\"\"Please summarize the following text in about 100 words while maintaining the key technical details:\n","\n","    {text}\"\"\"\n","\n","    return llm_model(prompt)\n","\n","def process_qa_result(question: str, qa_chain, llm_model, citation_format=\"APA\"):\n","    # Get QA result\n","    result = qa_chain({\"query\": question})\n","    answer = result[\"result\"]\n","    sources = result[\"source_documents\"]\n","\n","    # Extract helpful answer using regex\n","    pattern = r\"Helpful Answer:\\s*(.*?)(?=\\n\\n|Unhelpful Answer|$)\"\n","    match = re.search(pattern, answer, re.DOTALL)\n","    helpful_answer = match.group(1).strip() if match else \"No helpful answer found.\"\n","\n","    # Print question and answer\n","    print(f\"Question: {question}\")\n","    print(f\"Answer: {helpful_answer}\\n\")\n","\n","    # Process and print summarized source texts\n","    print(\"Summarized Source Texts:\")\n","    for i, doc in enumerate(sources, 1):\n","        source_text = doc.page_content.strip()\n","        page_number = doc.metadata.get('page', 'Unknown')\n","\n","        # Generate summary using LLM\n","        summary = summarize_text(source_text, llm_model)\n","\n","        print(f\"Source {i} (Page {page_number}):\")\n","        print(f\"Summary: {summary}\\n\")\n","\n","    # Generate and print single citation for the paper\n","    # Using the first source's metadata since all sources are from the same paper\n","    if sources:\n","        metadata_fetcher = PaperMetadata()\n","        paper_url = \"https://arxiv.org/pdf/1706.03762.pdf\"  # Transformer paper\n","        metadata = metadata_fetcher.get_paper_metadata(paper_url)\n","        citation = generate_citation(metadata, format=citation_format)\n","        print(f\"\\nPaper Citation:\")\n","        print(f\"{citation}\")\n","\n","# Example usage\n","question = \"What is the training complexity of the Transformer model compared to RNNs?\"\n","# Assuming qa_chain and llm_model are already initialized\n","process_qa_result(question, qa_chain, llm, citation_format=\"APA\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":367,"status":"ok","timestamp":1728498445279,"user":{"displayName":"Siddharth Deep Learning","userId":"08603031256831117826"},"user_tz":-330},"id":"wSi-7Hpr7Xi4","outputId":"ee4fa692-99bc-4807-a577-d7dc7a54b362"},"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting /content/drive/MyDrive/RAG Project/Streamlit Apps/app.py\n"]}],"source":["# -----------------------------------------------------------------------------\n","# Streamlit UI: Research Question, Citation Format, Answer, Citations, Download\n","# -----------------------------------------------------------------------------\n","%%writefile \"/content/drive/MyDrive/RAG Project/Streamlit Apps/app.py\"\n","import streamlit as st\n","import nbimporter\n","import pandas as pd\n","import re\n","import os\n","from helper_functions import load_pdfs, setup_qa_chain, generate_citation, generate_contextualized_citations, save_bibliography\n","from langchain.document_loaders import PyPDFLoader\n","\n","# Function to handle file upload and saving\n","def save_uploaded_file(uploaded_file, save_dir):\n","    \"\"\"\n","    Save the uploaded file to the specified directory\n","    Returns the path to the saved file\n","    \"\"\"\n","    if not os.path.exists(save_dir):\n","        os.makedirs(save_dir)\n","\n","    file_path = os.path.join(save_dir, uploaded_file.name)\n","    with open(file_path, \"wb\") as f:\n","        f.write(uploaded_file.getbuffer())\n","    return file_path\n","\n","# Streamlit App\n","st.title(\"Scientific Research Paper Summarization and Citation Generator\")\n","\n","# File upload section\n","st.subheader(\"Upload Research Paper\")\n","uploaded_file = st.file_uploader(\"Choose a PDF file\", type=['pdf'])\n","\n","# Directory setup\n","data_dir = \"/content/drive/MyDrive/RAG Project/Data\"  # Your existing data directory\n","\n","# Handle file upload\n","if uploaded_file is not None:\n","    try:\n","        # Save the uploaded file\n","        file_path = save_uploaded_file(uploaded_file, data_dir)\n","        st.success(f\"File {uploaded_file.name} successfully uploaded!\")\n","\n","        # Load the newly uploaded document\n","        loader = PyPDFLoader(file_path)\n","        new_document = loader.load()\n","\n","        # Add the new document to existing documents (if any)\n","        documents = []\n","        if os.path.exists(data_dir):\n","            documents = load_pdfs(data_dir)\n","    except Exception as e:\n","        st.error(f\"Error processing file: {str(e)}\")\n","\n","# Rest of your existing app code\n","question = st.text_input(\"Enter your research question:\")\n","\n","citation_format = st.selectbox(\n","    \"Select Citation Format\",\n","    (\"APA\", \"MLA\", \"Chicago\", \"BibTeX\", \"RIS\")\n",")\n","\n","if st.button(\"Generate Answer and Citations\"):\n","    if question:\n","        if not documents:\n","            documents = load_pdfs(data_dir)\n","\n","        # Set up the RAG system with FAISS and Hugging Face LLM\n","        qa_chain = setup_qa_chain(documents)\n","\n","        # Function to simulate the ask_question process\n","        def ask_question(question: str):\n","            result = qa_chain({\"query\": question})\n","            return result[\"result\"], result[\"source_documents\"]\n","\n","        # Generate answer and retrieve sources\n","        answer, sources = ask_question(question)\n","\n","        pattern = r\"Helpful Answer:\\s*(.*?)(?=\\n\\n|Unhelpful Answer|$)\"\n","        match = re.search(pattern, answer, re.DOTALL)\n","\n","        # If a helpful answer is found, extract it, otherwise print the entire answer\n","        if match:\n","            helpful_answer = match.group(1).strip()\n","        else:\n","            helpful_answer = \"No helpful answer found.\"\n","\n","        # Display the generated answer\n","        st.write(f\"**Answer:** {helpful_answer}\")\n","\n","        source_data = []\n","        for doc in sources:\n","            source_text = doc.page_content.strip()\n","            page_number = doc.metadata['page']\n","            source_data.append({\"Source Text\": source_text, \"Page Number\": page_number})\n","\n","        # Display source texts in a table\n","        source_df = pd.DataFrame(source_data)\n","        st.write(\"**Source Documents:**\")\n","        st.table(source_df)\n","\n","        # Button to allow users to see the source text in detail\n","        if st.button(\"Show Detailed Source Texts\"):\n","            for i, doc in enumerate(sources, start=1):\n","                source_text = doc.page_content.strip()\n","                page_number = doc.metadata['page']\n","                st.write(f\"**Source Text {i} (Page {page_number}):** {source_text}\")\n","\n","        # Generate and display contextualized citations\n","        contextual_citations = generate_contextualized_citations(sources, format=citation_format)\n","\n","        st.write(f\"**Citations in {citation_format} format with context:**\")\n","        for citation, context in contextual_citations:\n","            st.write(f\"- {citation}\")\n","            st.write(f\"  Context: {context}\\n\")\n","\n","        # Provide download option for BibTeX or RIS\n","        if citation_format in [\"BibTeX\", \"RIS\"]:\n","            bibliography = save_bibliography(sources, format=citation_format)\n","            st.download_button(\n","                label=f\"Download Citations in {citation_format} format\",\n","                data=bibliography,\n","                file_name=f\"bibliography.{citation_format.lower()}\",\n","                mime=\"text/plain\"\n","            )\n","    else:\n","        st.warning(\"Please enter a research question.\")\n","else:\n","    st.info(\"Upload a PDF and click 'Generate Answer and Citations' to begin.\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2095,"status":"ok","timestamp":1730968461810,"user":{"displayName":"Siddharth Deep Learning","userId":"08603031256831117826"},"user_tz":-330},"id":"YQU4gDxE9632","outputId":"e5468fe8-8d53-46db-b8be-d304b5145967"},"outputs":[{"output_type":"stream","name":"stdout","text":["Streamlit app running at: NgrokTunnel: \"https://9d29-34-125-39-91.ngrok-free.app\" -> \"http://localhost:8501\"\n"]}],"source":["from pyngrok import ngrok\n","ngrok.set_auth_token(\"2n945CdabNadYl9dMSbPMb98hBv_ckAXyHZVAh7TcEWoCDaV\") #account 1\n","# ngrok.set_auth_token(\"2nEGd89Rwyhngr0qPOb0NpxR1WM_2p5tMKZmwKd8chhuLmhpe\") #account 2\n","public_url = ngrok.connect(addr=\"8501\")\n","print(f\"Streamlit app running at: {public_url}\")"]},{"cell_type":"code","source":["from pyngrok import ngrok\n","\n","# Set your ngrok authentication token\n","ngrok.set_auth_token(\"2n945CdabNadYl9dMSbPMb98hBv_ckAXyHZVAh7TcEWoCDaV\") #account 1\n","# ngrok.set_auth_token(\"2nEGd89Rwyhngr0qPOb0NpxR1WM_2p5tMKZmwKd8chhuLmhpe\") #account 2\n","# Reserve a domain on ngrok dashboard and provide the hostname\n","public_url = ngrok.connect(addr=\"8501\", hostname=\"dog-vast-muskox.ngrok-free.app\") # for Account 1\n","# public_url = ngrok.connect(addr=\"8501\", hostname=\"deeply-pleasant-koi.ngrok-free.app\") # for Account 2\n","\n","print(f\"Streamlit app running at: {public_url}\")\n"],"metadata":{"id":"Vse2kuPjNvel","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1730221743738,"user_tz":-330,"elapsed":4867,"user":{"displayName":"Siddharth Deep Learning","userId":"08603031256831117826"}},"outputId":"4e63e7f6-de34-4a0c-a64c-057ab41093f1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Streamlit app running at: NgrokTunnel: \"https://dog-vast-muskox.ngrok-free.app\" -> \"http://localhost:8501\"\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UZF1X_mZ8Ojr"},"outputs":[],"source":["!streamlit run \"/content/drive/MyDrive/RAG Project/Streamlit Apps/app.py\" &>/dev/null&"]},{"cell_type":"code","source":[],"metadata":{"id":"4AJmPm8mMD6G"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"L4","machine_shape":"hm","provenance":[],"mount_file_id":"1SP8I4Bmm8yNOo0BaR4bR-ILvdlZsm-HI","authorship_tag":"ABX9TyN7ru3ZlcpgzN3/fSDN0nAi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"647a0920ab9344c49bdba3a1a1c4ecde":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4857b7e81d3b440b800b56f8ba0fc10d","IPY_MODEL_e77d334e3052478ba1f7cd5ff6fa97b5","IPY_MODEL_4bab21799b05459c89813656af8c9518"],"layout":"IPY_MODEL_cb5c5c02c0764dbcbb809b0e8e4d8448"}},"4857b7e81d3b440b800b56f8ba0fc10d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_176da66791014f8cb214145f7fb6d087","placeholder":"​","style":"IPY_MODEL_444a50209cc041f8a52e8a10bacf8412","value":"Loading checkpoint shards: 100%"}},"e77d334e3052478ba1f7cd5ff6fa97b5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c36b58d75951460894f0997f76b7a683","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_7deda925f3e44c139433cbc7acbc43d2","value":2}},"4bab21799b05459c89813656af8c9518":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_356b86620b2848748bcccc9fad9887bb","placeholder":"​","style":"IPY_MODEL_4a7bba48d71141c2a5065d82037c53cb","value":" 2/2 [00:05&lt;00:00,  2.60s/it]"}},"cb5c5c02c0764dbcbb809b0e8e4d8448":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"176da66791014f8cb214145f7fb6d087":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"444a50209cc041f8a52e8a10bacf8412":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c36b58d75951460894f0997f76b7a683":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7deda925f3e44c139433cbc7acbc43d2":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"356b86620b2848748bcccc9fad9887bb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a7bba48d71141c2a5065d82037c53cb":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}